%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
% \usepackage{subfigure}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{subcaption}
\input{macros}

\definecolor[named]{Blue}{cmyk}{1,0.1,0,0.1}
\definecolor[named]{Yellow}{cmyk}{0,0.16,1,0}
\definecolor[named]{Orange}{cmyk}{0,0.42,1,0.01}
\definecolor[named]{Red}{cmyk}{0,0.90,0.86,0}
\definecolor[named]{LightBlue}{cmyk}{0.49,0.01,0,0}
\definecolor[named]{Green}{cmyk}{0.20,0,1,0.19}
\definecolor[named]{Purple}{cmyk}{0.55,1,0,0.15}
\definecolor[named]{DarkBlue}{cmyk}{1,0.58,0,0.21}

\usepackage[acronym,smallcaps,nowarn,section,nogroupskip,nonumberlist]{glossaries}
\glsdisablehyper{}
\newacronym{SCFM}{scfm}{stochastic control-flow model}
\newacronym{WS}{ws}{wake-sleep}
\newacronym{BWS}{bws}{basic wake-sleep}
\newacronym{RWS}{rws}{reweighted wake-sleep}
\newacronym{ELBO}{elbo}{evidence lower bound}
\newacronym{VAE}{vae}{variational autoencoder}
\newacronym{IWAE}{iwae}{importance weighted autoencoder}
\newacronym{KL}{kl}{Kullback-Leibler}
\newacronym{SGD}{sgd}{stochastic gradient descent}
\newacronym{VIMCO}{vimco}{variational inference for Monte Carlo objectives}
\newacronym{WW}{ww}{wake-wake}
\newacronym{WWS}{wws}{wake-wake-sleep}
\newacronym{AIR}{air}{Attend, Infer, Repeat}
\newacronym{ESS}{ess}{effective sample size}
\newacronym{REINFORCE}{reinforce}{Reinforce gradient estimator}
\newacronym{IS}{is}{importance sampling}
\newacronym{GMM}{gmm}{Gaussian mixture model}
\newacronym{MNIST}{mnist}{hand-written digit dataset}
\newacronym{RELAX}{relax}{RELAX gradient estimator}
\newacronym{REBAR}{rebar}{REBAR gradient estimator}
\newacronym{PMF}{pmf}{probability mass function}
\newacronym{MLP}{mlp}{multilayer perceptron}
\newacronym{RNN}{rnn}{recurrent neural network}
\newacronym{PCFG}{pcfg}{probabilistic context free grammar}
\newacronym{ADAM}{adam}{ADAM}
\glsunset{ADAM}

\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newcommand{\given}{\lvert}
\newcommand{\pw}{\overset{\text{p.w.}}{\sim}
}
%%%%%%% hand-added %%%%%


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Amortized Population Gibbs Samplers with Neural Sufficient Statistics}

\begin{document}

\twocolumn[
\icmltitle{Amortized Population Gibbs Samplers with Neural Sufficient Statistics}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Wu}{neu}
\icmlauthor{Heiko Zimmermann}{neu}
\icmlauthor{Eli Sennesh}{neu}
\icmlauthor{Tuan Anh Le}{mit}
\icmlauthor{Jan-Willem van de Meent}{neu}
\end{icmlauthorlist}

\icmlaffiliation{neu}{Northeastern University, USA}
\icmlaffiliation{mit}{Massachusetts Institute of Technology, USA}

\icmlcorrespondingauthor{Hao Wu}{haowu@ccs.neu.edu}
\icmlcorrespondingauthor{Jan-Willem van de Meent}{jwvdm@ccs.neu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% We develop amortized population Gibbs (APG) samplers, a new class of autoencoding variational methods for deep probabilistic models. 
% APG samplers combine variational inference to learn approximate Gibbs conditionals for low-dimensional blocks of variables with MCMC methods to simulate high quality samples from the high-dimensional conditional posterior. %technically not 100% correct bacause we are not sampling from the exact conditional posterior. However the word 'simulate' is might still be okay here?
% Each conditional proposal is parameterized by a neural network which we train by minimizing the inclusive KL divergence relative to the conditional posterior. 
% To appropriately account for the size of the input data, we develop a new neural network parameterization in terms of neural sufficient statistics, resulting in quasi-conjugate variational approximations. 
% Experiments demonstrate that the learned proposals converge to the known analytical conditional posterior in conjugate models, and that APG samplers can learn inference networks for highly-structured deep generative models when the conditional posteriors are intractable. 
% Here APG samplers offer a path toward scaling up stochastic variational methods to models in which standard autoencoding architectures fail to produce accurate samples.


%Variational autoencoders are most commonly used to map input data onto a latent code in the form of a continuous vector. 

Amortized variational methods have proven difficult to scale to structured problems, such as inferring positions of multiple objects from video images. We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frames structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.

% , which enables us to learn a structured deep generative model in an unsupervised manner.

% Experiments show that APG samplers can produce accurate samples in high-dimensional structured problems, which enables us to learn a deep generative model in an unsupervised manner.
% Experiments show that we can train both the deep generative model and inference model in an unsupervised manner using APG methods
% Experiments establish that APG methods can produce accurate samples in high-dimensional structured problems where standard autoencoding variational methods fail.

%and can be used to train structured deep generative models in an unsupervised manner. %Here APG samplers offer a path toward scaling up stochastic variational methods to models in which standard autoencoding variational methods fail to produce accurate samples.


%Experiments demonstrate that learned proposals converge to the exact conditional posterior in conjugate models. 


%  We develop amortized population Gibbs (APG) samplers, a class of importance sampling methods that perform inference in high-dimensional problems by iterating between updates to lower-dimensional blocks of variables. We parameer 

% conditional proposals by minimizing the inclusive KL divergence relative to the conditional posterior. resulting in quasi-conjugate variational approximations. 
% In non-conjugate settings, 

%a new class of autoencoding variational methods for deep probabilistic models. APG samplers 
\end{abstract}
%%% We introduce APG samplers 

%%%
\section{Introduction}
%\vspace{-0.5em}

\label{introduction}

Deep probabilistic programming libraries such as Edward \cite{tran2016edward}, Pyro \cite{bingham2018pyro}, and Probabilistic Torch \cite{siddharth2017learning}  extend deep learning frameworks with functionality for deep probabilistic models which combine a generative model with an inference model that approximates the Bayesian posterior.
Both models are parameterized using neural networks, which are trained using stochastic gradient descent by optimize a lower or upper bound on the log marginal likelihood. Training an inference network to perform amortized inference can be equivalently understood as a form of variational inference or adaptive importance sampling.

At present, deep probabilistic models most commonly have the form of standard variational autoencoders (VAEs) \cite{kingma2013auto-encoding, rezende2014stochastic}. In these architectures, the  generative model combines an unstructured prior (e.g.~a spherical Gaussian) with a likelihood that is parameterized by an expressive neural network, often referred to as a decoder. The inference network, known as an encoder, maps input data (e.g.~an image or sentence) onto an embedding vector, also known as the latent code.

Deep probabilistic programming aims to enable more general designs that incorporate structured priors for tasks such as multiple object detection \cite{eslami2016attend}, language modeling \cite{esmaeili2019structured}, or object tracking \cite{kosiorek2018sequential}. In these domains, a prior can incorporate useful inductive biases, such as the requirement that object trajectories are smooth. These biases in turn can help guide a model to uncover patterns in the data in an unsupervised manner, and aid generalization in complex domains where the training data may not contain exemplars for all possible combinations of latent features.

However, training structured models also poses challenges that are not encountered in unstructured problems. To optimize a lower or an upper bound, we need to approximate the gradient of an expectation with a Monte Carlo estimate (see \cite{mohamed2019monte} for a recent review). Standard VAEs rely on reparameterized estimators that can often approximate the gradient with a single sample. Unfortunately, these estimators can have a high variance in models where latent variables are high-dimensional and/or strongly correlated. Owing to these limitations, models that are trained using standard VAE objectives often consider relatively small-scale problems, such as tracking $\le 2$ objects over the course of 10 frames \cite{kosiorek2018sequential}, or assigning $\le 10$ sentences in a review to distinct aspects \cite{esmaeili2019structured}. 

In this paper, we develop methods for amortized inference that are designed to scale to structured models with 100s of latent variables. We are particularly interested in the frequently arising cases of models that  are characterized by a combinations of \emph{local variables}, such as the time-dependent position of an object, and \emph{global variables}, such as the shape of the object. In this type of model, it is often the case that knowledge of the local variables can help us make predictions about global variables and vice versa; If we know the shape of an object, then it should be easier to identify its location in an image. Conversely, if we know the position of an object in each frame, then we can more readily infer its shape.

The methods that we develop in this paper are similar in spirit to work by Johnson et al.~\cite{johnson2016composing}, who developed methods for  conjugate-exponential models with a neural likelihood. In this setting, we can perform inference using variational expectation maximization (EM) algorithms \cite{beal2003variational,bishop2006pattern,wainwright2008graphical} that exploit conjugacy and conditional independence to derive closed-form updates to blocks of variables. The advantage of these approaches is that they are highly computationally efficient; variational EM can often converge in a small number of iterations and easily scales to much larger number of variables. Unfortunately, variational EM is also model-specific, difficult to implement, and only applicable to a restricted class of conjugate-exponential models.

To overcome the limitations imposed by conjugate-exponential family models, we here develop a more general approach. Rather than requiring exact EM updates we develop an importance sampling method that employs conditional proposals to iterate between updates to blocks of variables. To train these proposals, we define a a variational method that minimizes the inclusive KL divergence between the proposal update and the exact conditional posterior. We refer to the resulting class of methods as \emph{amortized Gibbs} samplers, since the proposals approximate Gibbs updates.

The variational objective that we derive is not computable, since the exact Gibbs updates are in general intractable. However, we can nonetheless derive a Monte Carlo estimator for its gradient. Building on a recent body of work that employs importance samplers to train variational distributions \cite{burda2016importance,le2018auto-encoding,maddison2017filtering,naesseth2018variational}, we develop a sequential Monte Carlo sampler \cite{delmoral2006sequential} that combines approximate Gibbs updates with resampling steps in order to construct high quality proposals, which serve both to compute gradient estimates at train time and to perform inference at test time. We demonstrate correctness of the proposed sampler by  proving that samples are properly weighted relative to the generative model \cite{naesseth2015nested}.

One of the challenges in designing networks that parameterize conditional proposals is network outputs need to appropriately account for the amount of data on which we are conditioning; The conditional posterior on the mean for a cluster with a large number of points is more tightly peaked than that of a cluster with a small number of points. To address this difficulty, we propose a class of networks that we refer to as \emph{neural sufficient statistics}, which define parameterizations of proposals in a manner that is additive in the local variables, much like the sufficient statistics in conjugate-exponential families.

Our experiments show that learned proposals converge to the true conditional posteriors in Gaussian mixture models, where the Gibbs updates can be computed in closed form. Moreover we establish that amortized Gibbs methods serve as a basis for scalable inference in structured deep generative models, including mixtures with neural likelihoods and unsupervised tracking models. Both of these tasks are representative of the current state-of-the art in unsupervised approaches for learning structured deep generative models.

\section{Amortized Population Gibbs Samplers}
\label{sec:amortized-gibbs}
We are interested in the task of jointly training a generative model $p_\q(x, z)$ by maximizing its marginal likelihood $p_\q(x)$ and learning an inference model $q_\f(z \mid x)$ that approximates the posterior $p_\q(z \mid x)$. Like most amortized inference approaches, we assume that we can sample from a (possibly implicit) distribution $\hat{p}(\x)$ that either takes the form of an empirical distribution over training data or a data simulator.

As a means of generating high-quality samples in an incremental manner, we develop methods that are inspired by expectation maximization and classic Gibbs sampling strategies, which perform iterative updates to blocks of variables. Concretely, we will assume that the latent variables in the generative model decompose into blocks $\z = \{\z_1, \ldots, \z_B\}$ and train proposals $q_\f(z_b \mid x, z_{-b})$ that update the variables in a each block $z_{b}$ conditioned on the variables in the remaining blocks $\z_{-b} = z \setminus \{z_b\}$.

Starting with an initial sample $q_\f(\z^{1} \mid \x)$ from a standard encoder we will generate a sequence of samples $\{z^1, \ldots, z^K\}$ by performing conditional updates to each block $z_b$, which we refer to as a \emph{sweep}
\begin{align}
    \label{eq:approx-gibbs-kernel}
    q_\f \big( \z^k \mid \x, \z^{k-1} \big)
    &=
    \prod_{b=1}^B
    q_\f(\z^k_b \mid \x, \z^{k}_{\prec b}, \z^{k-1}_{\succ b})
    ,
\end{align}
where $\z_{\prec b} = \{z_i \mid i < b\}$ and $\z_{\succ b} = \{z_i \mid i > b\}$. Repeatedly applying sweep updates then yields a proposal
\begin{equation*}
    q_\f(z^1, \ldots, z^K \mid x) 
    =
    q_\f(z^1 \mid x)
    \prod_{k=2}^K
    q_\f(z^k \mid \x, z^{k-1}).
\end{equation*}
We want to train proposals that improve the quality of each sample $z^k$ relative to that of the preceding sample $z^{k-1}$. There are two possible strategies for accomplishing this. One strategy is to define an objective that minimizes the discrepancy between the marginal $q_\f(z^K \mid x)$ for the final sample and the posterior $p_\q(z^K \mid x)$. This corresponds to learning a sweep update $q_\f(z^k \mid x, z^{k-1})$ that transforms the initial proposal to the posterior in exactly $K$ sweeps. An example of this type of approach, albeit one that does not employ block updates, is the recent work on annealing variational objectives \cite{huang2018improving}.

In this paper, we will pursue a different approach. Instead of transforming the initial proposal in exactly $K$ steps, we learn a sweep update that leaves the target density \emph{invariant}
\begin{equation}
    \label{eq:kernel-invariance}
    p_\q(\z^k \,|\, \x) = \int \: p_\q(\z^{k-1} \,|\, \x) \: q_\f(\z^k \,|\,\x, \z^{k-1}) d z^{k-1}.
\end{equation}
When this condition is met, the proposal $q_\f(z^1, \ldots z^K \,|\, x)$ is a Markov Chain whose stationary distribution is the posterior. 
This means a sweep update learned at training time can be applied at test time to iteratively improve sample quality, without requiring a pre-specified number of updates $K$.


In addition, when we require that each single block update $q_\f(z'_b \mid x, z_{-b})$ also leaves the target density invariant,
\begin{align}
    \label{eq:gibbs-invariance}
    p_\q(z'_b, z_{-b} \mid x)
    &= 
    \int 
    p_\q(z_b, z_{-b} \mid x) \:
    q_\f(z'_b, \mid x, z_{-b}) 
    dz_{b},
    \\
    &= 
    p_\q(z_{-b} \mid x) \:
    q_\f(z'_b \mid x, z_{-b})
    , \nonumber
\end{align}
Then we see that a block update must equal the exact conditional posterior, $q_\f(z'_b \mid x, z_{-b}) = p_\q(z'_b \mid x, z_{-b})$.
In other words, when the condition in Equation~\ref{eq:gibbs-invariance} is met, the proposal $q_\f(z^1, \ldots z^K \,|\, x)$ is a Gibbs sampler.


\subsection{Variational Objective} 
To learn each of the block proposals $q_\f(z_b \mid x, z_{-b})$ we will minimize the inclusive KL divergence $\mathcal{K}_b(\f)$
\begin{align}
    \E_{\hat{p}(x)p_\q(z_{-b} | x)}
    \left[
    \text{KL}\left(
        p_\q(z_{b} \mid x, z_{-b})
        ||
        q_\f(z_{b} \mid x, z_{-b})
    \right)
    \right]. \label{eq:variational_objective}
\end{align}
Unfortunately, this objective is intractable, since we are not able to evaluate the density of the true marginal $p_\q(z_{-b} \mid x)$, nor that of the conditional $p_\q(z_{b} \mid z_{-b}, x)$. 
% This also prevents us from computing an upper bound to $\log p_\theta(x)$ which is a proxy for the model quality.
As we will discuss in Section~\ref{sec:experiments}, this has implications for the evaluation of learned proposals, since we cannot compute a lower or upper bound on the log marginal likelihood as in other variational methods. However, it nonetheless possible to approximate the gradient of the objective 
\begin{align*}
    -\nabla_\f \mathcal{K}_b(\f)
    % &=
    % \E_{
    % p_\q(z_{b} | x, z_{-b}) p_\q(z_{-b} | x)
    % }
    % \!\!
    % \left[
    % \nabla_\f
    % \log q_\f(z_{b} \,|\, x, z_{-b})
    % \right]\\
    &=
    \E_{
    \hat{p}(x) \:
    p_\q(z_{b}, z_{-b} | x)
    }
    \left[
    \nabla_\f
    \log q_\f(z_{b} \,|\, x, z_{-b})
    \right].
\end{align*}
We can estimate this gradient using any Monte Carlo method that generates samples $z \sim p_\q(z \mid x)$ from the posterior. In the next section, we will use the learned proposals to define an importance sampler, which we then use to compute an self-normalized estimator of the gradient from weighted samples $\{(w^l, z^l)\}_{l=1}^L$, 
\begin{align}
    \label{eq:grad-self-normalized}
    -\nabla_\f \mathcal{K}_b(\f)
    \simeq
    \sum_{l=1}^L
    \frac{w^l}{\sum_{l'} w^{l'}}
    \nabla_\f
    \log q_\f(z^l_{b} \,|\, x, z^l_{-b})
    .
\end{align}
In problems where we would like to learn a deep generative model $p_\q(\x, \z)$, we can apply a similar self-normalized gradient estimator of the form
\begin{align}
    \nabla_\q \log p_\q(\x) 
    &=
    \mathbb{E}_{p_\q(\z | \x)} 
    \left[
    \nabla_\q \log p_\q(\x, \z)
    \right] \label{eq:grad-theta}\\
    &
    \simeq
    \sum_{l=1}^L
    \frac{w^l}{\sum_{l'} w^{l'}}
    \nabla_\q
    \log p_\q(x, z^l)
    .\nonumber
\end{align}
This identity holds due to the standard property $\E_{p_\q(z|x)}[\nabla_\q \log p_\q(z|x)]=0$ (see Appendix \ref{appendix:grad-theta} for details). 

The estimator in Equation~\ref{eq:grad-self-normalized} is similar to the self-normalized estimator in reweighted wake-sleep methods \cite{bornschein2014reweighted}, which also minimizes an inclusive KL divergence. This estimator has a number of advantages over the estimator that is commonly used to train standard VAEs, which minimize an exclusive KL divergence \cite{le2019revisiting}. Standard VAE objectives rely on reparameterization to compute gradient estimates. For discrete variables, reparameterization is not possible. This means that we need to compute likelihood-ratio estimators (also known as REINFORCE-style estimators \cite{williams1992simple}), which can have very high variance. A range of approaches for variance reduction have been put forward, including continuous relaxations that are amenable to reparameterization \cite{maddison2017concrete,jang2017categorical}, credit assignment techniques (see \cite{weber2019credit} for a review), and other control variates \cite{mnih2016variational,tucker2017rebar,grathwohl2018backpropagation}. 

The estimator in Equation~\ref{eq:grad-self-normalized} sidesteps the need for these variance reduction techniques. To compute this gradient, we only require that the proposal \emph{density} is differentiable, whereas reparameterized estimators require that the \emph{sample} itself is differentiable. This is a milder condition, that holds for most distributions of interest, including those over discrete variables. Moreover, since this estimator minimizes the inclusive KL divergence, and not the exclusive KL divergence, there is smaller risk of learning a proposal that collapses to a single mode of a multi-modal posterior \cite{le2019revisiting}.

\subsection{Generating High Quality Samples}
Approximating the gradient presents a chicken-and-egg problem; we need samples from the posterior to compute a Monte Carlo estimate of the gradient, but generating these samples is precisely what we are hoping to use learned proposals for in the first place. Moreover, self-normalized importance samplers are consistent, but they are not unbiased. In the early stages of training, we will have poor quality proposals, which means that the bias of the gradient estimators in Equations~\ref{eq:grad-self-normalized} and~\ref{eq:grad-theta} can be high. 
%If we can improve the quality of the importance sampling mechanism, regardless of the quality of the proposal, then this will improve accuracy of both inference and gradient estimation.
%may not be able to compute high-quality gradient estimates until we have good proposals, but we will not be able to learn the proposals without high-quality gradient estimates.

Standard reweighted wake-sleep methods generate proposals from an encoder $z \sim q_\f(z \mid x)$ and compute weights $w = p_\q(x, z) / q_\f(z \mid x)$. A well-known limitation of this type of naive importance sampling strategy is that the computed weights will have a very high variance in models with high-dimensional and/or correlated latent variables, which in turn implies a high bias of the estimator. There is a very broad class of importance sampling strategies that can be employed to reduce the variance of importance weights. If we replace the naive importance sampler in reweighted wake-sleep with a more sophisticated sampling strategy, then this both improves the quality of gradient estimates at training, and the quality of inference results at test time.

To improve upon standard reweighted wake-sleep methods, we will use the learned proposals to define a sequential Monte Carlo (SMC) sampler \cite{delmoral2006sequential}. %SMC methods reduce the variance of importance weights by decomposing a high-dimensional sampling problem into a sequence of lower-dimensional problems \cite{}. 
SMC methods \cite{doucet2001sequential} combine two basic ideas. The first is sequential importance sampling, which decomposes a proposal for a sequence of variables into a sequence of conditional proposals. The second is resampling, which selects partial proposals with probability proportional to their weights in order to improve the overall sample quality. Most commonly, SMC methods are used in the context of state space models to generate proposals for a sequence of variables by proposing one variable at a time. SMC \emph{samplers} (see  Algorithm~\ref{alg:smcs}) are a subclass of SMC methods that interleave resampling with the application of a transition kernel, which is sometimes also referred to as \emph{resample-move} SMC. 

The distinction between SMC methods for state space models and SMC samplers is subtle but important. Whereas the former generate proposals for a sequence of variables $z_{1:t}$ by proposing $z_t \sim q(z_t \mid z_{1:t-1})$ to \emph{extend} the sample space at each iteration, SMC samplers can be understood as an importance sampling analogue to Markov chain Monte Carlo (MCMC) methods \cite{brooks2011handbook}, which construct a Markov chain $z^{1:k}$ by generating a proposal $q(z^k \mid z^{k-1})$ from a transition kernel at each iteration.

\textbf{Sequential Importance Sampling}. To understand how approximate Gibbs proposals can be used in a SMC sampler, we will first explain how they can be used to define a sequential importance sampler, which decomposes the importance weight into a sequence of \emph{incremental} weights. In general, SIS considers a sequence of unnormalized target densities $\gamma^1(z^1), \gamma^2(z^{1:2}), \dots, \gamma^K(z^{1:K})$. If we now consider an initial proposal $q^1(z^1)$, along with a sequence of conditional proposals $q^k(z^k \mid z^{1:k-1})$, then we can recursively construct a sequence of weights $w^k = v^k w^{k-1}$ by assuming $w^1 = \gamma^1(z^1) / q^1(z^1)$ and defining the incremental weight
\begin{align*}
    v^k 
    &=
    \frac{\gamma^k(z^{1:k})}{\gamma^{k-1}(z^{1:k-1}) q^k(z^k \mid z^{1:k-1})}.
\end{align*}
This construction ensures that, at step $k$ in the sequence, we have a weight $w^k$ relative to the intermediate  density $\gamma^k(z^{1:k})$ of the form (see Appendix~\ref{appendix:sis-weight})
\begin{align*}
    w^k
    = 
    \frac{\gamma^k(z^{1:k})}
         {q^1(z^1) \prod_{k'=2}^k q^{k'}(z^{k'} \mid z^{1:k'-1})}.
\end{align*}
\begin{algorithm}[!t]
\setstretch{1.2}
  \caption{SMC sampler}
  \label{alg:smcs}
\begin{algorithmic}[1]
    \small
    \For{$l = 1$ \textbf{to} $L$}
        \State $z^{1,l} \sim q^1(\cdot)$\Comment{Propose}
        \State $w^{1,l} = \frac{\gamma^1(z^{1,l})}{q^1(z^{1,l})}$\Comment{Weigh}
    \EndFor
    \For{$k = 2$ \textbf{to} $K$}
      \State$z^{k-1,1:L}, w^{k-1,1:L}=\textsc{resample}(z^{k-1,1:L}, w^{k-1,1:L})$
    %   \State$\{z^{k-1,l}, w^{k-1,l}\}_{l=1}^L=\textsc{resample}(\{z^{k-1,l}, w^{k-1,l}\}_{l=1}^L)$

      \For{$l = 1$ \textbf{to} $L$}
          \State $z^{k,l} \sim q^k(\cdot \mid z^{k-1,l})$\Comment{Propose}\label{line:apg-propose}
          \State $w^{k,l} = \frac{\gamma^k(z^{k,l}) r^{k-1}(z^{k-1,l} \mid z^{k,l})}{\gamma^{k-1}(z^{k-1,l}) q^k(z^{k,l} \mid z^{k-1,l})}w^{k-1,l}$\Comment{Weigh}
      \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}
We will now consider a specific sequence of intermediate densities that are defined using a reverse kernel $r(z' \mid z)$
\begin{align*}
    \gamma^k(z^{1:k})
    =
    p_\q(x,z^k) \prod_{k'=2}^{k} r(z^{k'-1} \mid z^{k'})
    .
\end{align*}
This defines a density on an \emph{extended space} such that 
\begin{align*}
    \gamma^k(z^k) = \int \gamma^k(z^{1:k}) \: dz^{1:k-1} = p_\q(x, z^k).
\end{align*}
This means that at each step $k$, we can treat the preceding samples $z^{k-1}$ as \emph{auxiliary variables}; if we generate a proposal $z^{1:k}$ and simply disregard $z^{1:k-1}$, then the pair $(w^k, z^k)$ is a valid importance sample relative to $p_\q(z^k \mid x)$. If we additionally condition proposals on $x$, the incremental weight for this particular choice of target densities is
\begin{align}
    \label{eq:incremental-weight-forward-reverse}
    v^k 
    = 
    \frac{p_\q(x,z^k) \: r(z^{k-1} \mid  z^k)}
         {p_\q(x,z^{k-1}) \: q(z^{k} \mid  z^{k-1})}
    .
\end{align}
This construction defines a valid importance sampler for \emph{any} choice of proposal kernel $q(z^k \mid z^{k-1})$ and reverse kernel $r(z^{k-1} \mid z^{k})$. For a given choice of proposal, the \emph{optimal} reverse kernel is
\begin{align*}
    r(z^{k-1} \mid  z^k)
    =   
    \frac{p_\q(x,z^{k-1})}
         {p_\q(x,z^k)}
    q(z^{k} \mid  z^{k-1})
    .
\end{align*}
For this choice of kernel, the incremental weights are 1, which minimizes the variance of the weights $w^k$.

We will now use the approximate Gibbs kernel from Equation~\ref{eq:approx-gibbs-kernel} as both the forward and the reverse kernel
\begin{align}
    q(z^{k} \mid  z^{k-1}) = r(z^{k-1} \mid  z^k) = q_\f(z^k \mid x, z^{k-1}).
\end{align}
When the approximate Gibbs kernel converges to the actual Gibbs kernel, this choice becomes optimal, since the kernel will satisfy detailed balance in this limit
\begin{align*}
    p_\q(x, z^k) 
    \,
    q_\f(z^{k-1} \,|\, x, z^{k})
    =
    p_\q(x, z^{k-1}) 
    \,
    q_\f(z^{k} \,|\, x, z^{k-1}).
\end{align*}
\textbf{Resampling}. In general, the weights $w^k$ in the sequential importance sampling scheme defined above will have a high variance. The weights $w^1$ are just normal importance sampling weights, which themselves will have a high variance when $z$ is high-dimensional, or there are correlations between variables. Moreover, we are now sampling these same variables $k$ times. When the approximate Gibbs kernel converges to the true kernel, this will not increase the variance of weights (since $v^k=1$ in this limit), but during training variance of weights $w^k$ will increase with $k$, since we are now jointly sampling an entire Markov chain.

To overcome this problem, SMC samplers interleave application of the transition kernel with a \emph{resampling} step (see Appendix~\ref{appendix:resample-algo}). This step generates a new set of samples by selecting current samples with replacement, with probability proportional to their weight. Concretely, suppose that we have a set \emph{incoming} samples $\{(w^{k,l}, z^{k,l})\}_{l=1}^L$, then the resampling procedure (see Algorithm~\ref{alg:resample}) selects index $a$ with probability $P(a\!=\!l) = w^{k,l} / \sum_{l'} w^{k,l'}$ and returns an outgoing sample $z'^{k,l} = z^{k,a}$ whose $w'^{k,l} = \frac{1}{L} \sum_l w^{k,l}$ is equal to the average weight. The reduces the variance of the importance weights at the expense of also reducing the diversity of the sample set; high-weight samples are selected frequently, whereas low-weight samples are selected infrequently or not at all.

When we perform resampling after each sweep, we reduce the variance of importance weights to an extent. However we will likely still have high-variance weights, since each sample from the approximate Gibbs kernel still constitutes a high-dimensional proposal over all variables in the model. To further reduce the variance, we will employ resampling after each block update, rather than after each sweep. Because the incoming weights are now equal at each block update, we can compute gradient estimates using incremental weights $v$ of the form
\begin{align}
    v
    = 
    \frac{p_\q(x, z'_b, z_{-b}) \: q_\f(z_b \mid  x, z_{-b})}
         {p_\q(x, z_b, z_{-b}) \: q_\f(z'_b \mid  x, z_{-b})}
    .
\end{align}
These incremental weights will have a much lower variance than the incremental weights for a full sweep, since we are now able to decompose a sampling problem for all the variables in a model into sampling problems for individual blocks. In models with many latent variables, such as the ones that we will consider in our experiments, this has the potential to greatly increase the tractability of the gradient estimation problem. 


We refer to this implementation of a SMC sampler as an amortized population Gibbs (APG) sampler, and summarize all the steps of the computation in Algorithm~\ref{alg:amortized-gibbs}. In Appendix~\ref{appendix:proof-algo}, we prove that this algorithm is correct using an argument based on proper weighting. More informally this property holds due to the fact that this sampler is a specific instance of a SMC sampler. 

\begin{algorithm}[!tb]
\setstretch{1.2}
  \caption{Amortized Population Gibbs Sampling}
  \label{alg:amortized-gibbs}
\begin{algorithmic}[1]
\small
  \State $g_\phi = 0, g_\theta = 0$\Comment{Initialize gradient to 0}\label{line:init-grad}
  \For{$l = 1$ \textbf{to} $L$}\label{line:rws-loop}\Comment{Initial proposal}
      \State $z^{1,l} \sim q_\phi(\cdot \mid x)$\Comment{Propose}\label{line:rws-propose}
      \State $w^{1,l} = \frac{p_\theta(x, z^{1,l})}{q_\phi(z^{1,l} \mid x)}$\Comment{Weigh}\label{line:rws-weight}
  \EndFor
  \State $g_\phi = g_\phi + \sum_{l = 1}^L \frac{w^{1,l}}{\sum_{l' = 1}^L w^{1,l'}} \nabla_\f \log q_\phi(z^{1,l} \mid x)$\label{line:rws-grad-phi}
  \State $g_\theta = g_\theta + \sum_{l = 1}^L \frac{w^{1,l}}{\sum_{l' = 1}^L w^{1,l'}} \nabla_\q \log p_\theta(x, z^{1,l})$\label{line:rws-grad-theta}
  \For {$k = 2$ \textbf{to} $K$}\label{line:sweep-loop}\Comment{Gibbs sweeps}
    \State $\tilde{z}^{1:L}, \tilde{w}^{1:L} = z^{k-1,1:L}, w^{k-1,1:L}$ \label{line:apg-sweep-begin}
    \For{$b = 1$ \textbf{to} $B$}\label{line:block-loop}\Comment{Block updates}
      \State $\tilde{z}^{1:L}, \tilde{w}^{1:L}=\textsc{resample}(\tilde{z}^{1:L}, \tilde{w}^{1:L})$\label{line:resample} 
        \For{$l = 1$ \textbf{to} $L$}\label{line:apg-sample-loop}
          \State $\tilde{z}'^{\:l}_b \sim q_\phi(\cdot \mid x, \tilde{z}_{-b}^l)$\Comment{Propose}\label{line:apg-propose}
          \State \label{line:apg-weight} $\tilde{w}^l = \frac{p_\theta(x, \tilde{z}'^{\:l}_{b}, \tilde{z}^{\:l}_{-b}) q_\f(\tilde{z}^{\:l}_b \mid x, \tilde{z}^{\:l}_{-b})}{p_\theta(x, \tilde{z}^{\:l}_b, \tilde{z}^{\:l}_{-b}) q_\phi(\tilde{z}'^{\:l}_b \mid x, \tilde{z}^{\:l}_{-b})}\tilde{w}^l$ \Comment{Weigh}
          \State \label{line:apg-reassign}$\tilde{z}^{\:l}_b = \tilde{z}'^{\:l}_b$ \Comment{Reassign}
      \EndFor
      \State $g_\phi=g_\phi+ \sum_{l = 1}^L\frac{\tilde{w}^l}{\sum_{l'= 1}^L \tilde{w}^{l'}} \nabla_\f\log q_\phi(\tilde{z}^{\:l}_b \mid x, \tilde{z}^{\:l}_{-b})$\label{line:apg-grad-phi}
      \State $g_\theta = g_\theta + \sum_{l = 1}^L \frac{\tilde{w}^l}{\sum_{l' = 1}^L \tilde{w}^{l'}} \nabla_\q \log p_\theta(x, \tilde{z}^l)$ \label{line:apg-grad-theta}
     \EndFor
     \State $z^{k,\:1:L}, w^{k,\:1:L} = \tilde{z}^{1:L}, \tilde{w}^{1:L}$\label{line:apg-sweep-end}
     \vspace{0.5em}
  \EndFor
  \Return $g_\phi$, $g_\theta$\Comment{Output: gradient}
\end{algorithmic}
\end{algorithm}
\section{Neural Sufficient Statistics}
Gibbs sampling strategies that sample from exact conditionals rely on conjugacy relationships. Typically, we assume a prior and likelihood that can both be expressed as exponential families
\begin{align*}
    p(\x \mid \z) 
    &= 
    h(\x) \exp \{ 
        \eta(\z)^\top \: T(\x)  
        -\log A(\eta(\z)) \}, 
    \\
    p(\z ) 
    &= 
    h(\z) \exp \{ 
        \lambda^\top T(\z) 
        - \log A(\lambda) \}.
\end{align*}
In these densities $h(\cdot)$ is a base measure, $T(\cdot)$ is a vector of sufficient statistics, and $A(\cdot)$ is a log normalizer. The two densities are jointly conjugate when
\begin{align*}
    T(\z) = (\eta(\z), -\log A(\eta(\z)))
\end{align*}
In this case, the posterior distribution lies in the same exponential family as the prior
\begin{align*}
    p(\z \mid \x) 
    \propto 
    h(z)
    \exp 
    \big\{
        & (\lambda_1 + T(\x))^\top T(\z) 
        \\
        &
        -
        (\lambda_2 + 1) 
        \log A(\eta(\z))
    \big\}
    .
\end{align*}
Typically, the prior $p(\z \mid \lambda)$ and likelihood $p(x \mid z)$ are not jointly conjugate, but it is possible to identify conjugacy relationships at the level of individual blocks of variables, 
\begin{align*}
    p(\z_b \mid \z_{-b}, \x)
    \propto
    h(z_b) 
    \exp \big\{ 
        &
        (\lambda_{b,1} + T(\x, \z_{-b}))^\top T(\z_b) 
        \\
        &
        -
        (\lambda_{b,2} \!+\! 1) 
        \log A(\eta(\z_b))
    \big\}
    .
\end{align*}
In the more general setting we consider here, these conjugacy relationships will typically not hold. However, we can still take inspiration to design variational distributions that make use of conditional independencies in a model. We will assume that each of the approximate Gibbs updates $q_\f(\z_b \mid x, \z_{-b})$ is an exponential family, whose parameters are computed from a vector of prior parameters $\lambda$ and a vector of neural sufficient statistics $T_\f(\x, \z_{-b})$
\begin{align}
    q_\f(\z_b \mid x, z_{-b}) 
    = 
    p(\z_b \mid \lambda + T_\f(x, z_{-b}))
    .
\end{align}
This parameterization has a number of desirable properties. Exponential families are the largest-entropy distributions that match the moments defined by the sufficient statistics (see e.g.~\cite{wainwright2008graphical}), which is helpful when minimizing the inclusive KL divergence. In exponential families it is also more straightforward to control the entropy of the variational distribution. In particular, we can initialize $T_\f(x, z_{-b})$ to output values close to zero in order to ensure that we initially propose from a prior and/or regularize $T_\f(x, z_{-b})$ to help avoid local optima.

A particularly useful case arises in models where the data $\x = \{x_1, \ldots, x_N\}$ are independent conditioned on $\z$. In these models it is often possible to partition the latent variables $\z = \{\z^\textsc{g}, \z^\textsc{l}\}$ into global and local variables $\z^{\textsc{g}}$ and local variables $\z^{\textsc{l}}$. The dimensionality of global variables is typically constant, whereas local variables $\z^\textsc{l} = \{\z^\textsc{l}_1, \ldots, \z^\textsc{l}_N\}$ have a dimensionality that increases with the data $N$. For models with this structure, the local variables are typically conditionally independent $z^\textsc{L}_n \bot z^\textsc{L}_{-n} \mid x, z^\textsc{g}$, which means that we can parameterize the sufficient statistics as
\begin{align*}
    \tilde{\lambda}^\textsc{g}
    &= 
    \lambda^\textsc{g} 
    + 
    \sum_{n=1}^N 
    T^\textsc{g}_\f(\x_n, z^\textsc{l}_{n}),
    &
    \tilde{\lambda}^\textsc{l}_n
    &=
    \lambda^\textsc{l}_n + T^\textsc{l}_\f(\x_n, \z^\textsc{g}).
\end{align*}
The advantage of this parameterization is it allows us to train approximate Gibbs updates for global variables in a manner that scales dynamically with the size of the dataset, and appropriately adjusts the posterior variance according to the amount of available data.
% \begin{figure*}[t!]
% \centering
%   \includegraphics[width=2\columnwidth]{figures/gmm_samples_and_baselines.pdf}
% \end{figure*}
\begin{figure*}[t!]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  \includegraphics[width=85mm]{figures/gmm_samples.pdf}
  \vspace*{-1mm}
  \caption{GMM}
  \label{samples-gmm}
  \vspace{-1ex}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
  \includegraphics[width=85mm]{figures/dgmm_samples.pdf}
  \vspace*{-1mm}
  \caption{DGMM}
  \label{samples-dgmm}
  \vspace{-1ex}
  \end{subfigure}
  \caption{Single samples in both (\textbf{a}) GMM and (\textbf{b}) DGMM. The left column shows one randomly-drawn test dataset, The subsequent columns show inference results by RWS, followed by results after different number of APG updates. The right column in (\textbf{b}) DGMM additionally shows reconstructions from the learned generative model.}
  \label{samples-mixture}
\end{figure*}



\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/gmm_blocks_comparison.pdf}
  \caption{Comparison with with differnt block strategies under the same computation budget.}
  \label{fig:convergence-gmm}
\end{figure}

% \begin{figure}[t!]
% \centering
% \includegraphics[width=\columnwidth]{figures/convergence_gmm.pdf}
%   \caption{Comparison with baselines}
%   \label{fig:convergence-gmm}
% \end{figure}

% \begin{figure}[t!]
%   \centering
%   \begin{subfigure}[t]{0.5\columnwidth}
%   \includegraphics[width=40mm]{figures/heatmap_log_joint_gmm.pdf}
%   \vspace*{-1mm}
%   \caption{GMM}
%   \label{fig:heatmap-gmm}
%   \vspace{-1ex}
%   \end{subfigure}%
%   \begin{subfigure}[t]{0.5\columnwidth}
%   \includegraphics[width=40mm]{figures/heatmap_log_joint_dgmm.pdf}
%   \vspace*{-1mm}
%   \caption{DGMM}
%   \label{fig:heatmap-dgmm}
%   \vspace{-1ex}
%   \end{subfigure}
%   \caption{Heatmap of $\log p_\theta(x, z)$ (the test sets in Figure~\ref{samples-mixture}) with different combinations of number of APG sweeps $K$ and number of particles $L$. The main diagonal elements (highlighted by green boxes) are results under same computation budget $K \cdot L = 1000$.}
%   \label{fig:heatmap-both}
% \end{figure}

\section{Related Work}
Our work fits into a line of recent methods for deep generative modeling that seek to improve inference quality, either by introducing auxiliary variables~\cite{maaloe2016auxiliary, ranganath2016hierarchical}, or by performing iterative updates~\cite{ marino2018iterative}. Our specific approach to learning block proposals is related to a number of methods that, in some way or other, combine transition kernels with variational inference. Work by  Hoffman~\cite{hoffman2017learning} applies Hamiltonian Monte Carlo to samples that are generated from the encoder, which serves to improve the gradient estimate w.r.t.~$\theta$ (Equation~\ref{eq:grad-theta}), while learning the inference network using a standard reparameterized ELBO objective. 
% Li et al.~\cite{li2017approximate} consider learning an inference network which is used for initializing an MCMC chain. They focus on a setting where the density of the inference network cannot be evaluated.
% Learning is also performed by targeting the inclusive KL. However, the posterior expectation is approximated using MCMC and, due to the inability to compute the density, the gradient is estimated adversarially. 
Li et al.~\cite{li2017approximate} similarly use MCMC to improve the quality of samples that are generated by an encoder, but additionally use these samples to train the encoder by minimizing the inclusive KL divergence relative to the filtering distribution of the Markov chain. As in our work, the filtering distribution after multiple MCMC steps is intractable. Li et al.~therefore use an adversarial objective to minimize the inclusive KL. Neither of these two lines of work consider block decomposition of the latent variable space, nor do they learn transition kernels. 
 
Work by Salimans et al.~\cite{salimans2015markov} uses transition kernels in variational inference. The authors use an importance weight to define (stochastic) lower bound, which is defined using a forward and reverse kernel in the same manner as in Equation~\ref{eq:incremental-weight-forward-reverse}. Huang et al.~\cite{huang2018improving} extend the work by Salimans et al.~by learning a sequence of transition kernels that performs annealing from the initial encoder to the posterior. Since both these methods minimize an exclusive KL, rather than an inclusive KL, gradient estimates must be computed using reparameterization, which means that these methods are not applicable to models that contain discrete variables. Moreover, these methods perform a joint update on all variables at each iteration, and do not consider the task of learning conditional proposals as we do here.

%% russell amortized MCMC https://papers.nips.cc/paper/7669-meta-learning-mcmc-proposals.pdf
%% - learns block conditionals using sleep loss
%% - focus on conditional-sleep
Work by Wang et al.~\cite{wang2018meta} develops a meta-learning approach to learning Gibbs block conditionals. This work assumes a setup in which it is possible to sample $x, z \sim p(x, z)$ from the true generative model $p(x,z)$, which means gradients can be estimated using sleep-phase Monte Carlo estimators. This circumvents the need for self-normalized estimators of the form in Equation~\ref{eq:grad-self-normalized}, which are necessary when we additionally wish to learn the generative model. Like in our work, the approach by Wang et al.~minimizes the inclusive KL, but uses the learned conditionals to directly define an (approximate) MCMC sampler, rather than using them as proposals in an SMC sampler. This work also has a somewhat different focus from ours, in that it primarily seeks to learn block conditionals that have the potential to generalize to previously unseen graphical models.


\section{Experiments}
\label{sec:experiments}

We evaluate APG methods in 3 tasks. We begin by considering a Gaussian mixture model (GMM) as an exemplar of a model in the conjugate-exponential family. Here we verify that the learned block updates converge the analytical conditional posteriors as predicted by our analysis in Section~\ref{sec:amortized-gibbs}. We next consider a deep generative mixture model (DGMM) that incorporates a neural likelihood to parameterize ring-shaped clusters. We show that we can train both the generative model and inference model in an end-to-end manner using APG methods, and that inference scales to datasets containing up to 600 points. For both models we quantify performance in terms of the effective sample size (ESS) and the relative magnitude of the log joint. In our third experiment, we consider an unsupervised model for multiple bouncing MNIST data. We extend the task proposed by Srivastava et al.~\cite{srivastava2015unsupervised} to consider up to 5 individual digits, and learn both a deep generative model for videos and an inference model that performs tracking.

% \setlength{\tabcolsep}{4pt}
% \begin{table}[t!]
% \centering
% \caption{$\log p_\theta(x, z)$ and ESS (the test sets in Figure~\ref{samples-mixture}) for GMM as a function of various numebr of sweeps under the same computation budget computation budget $K \cdot L = 1000$.}
% \begin{tabular}{cccccc}
% \toprule
% K & 1 & 5 & 10 & 20 & 50  \\
% L & 1000 & 200 & 100 & 50 & 20 \\
% \midrule
%     $\log p_\theta(x, z)$ & -722.2 & -523.6 & -453.7 & -426.9 & -425.8 \\
%     ESS / L & 0.002 & 0.890 & 0.902 & 0.995 & 0.995 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \setlength{\tabcolsep}{4pt}
% \begin{table}[t!]
% \centering
% \caption{$\log p_\theta(x, z)$ and ESS (the test sets in Figure~\ref{samples-mixture}) for DGMM as a function of various numebr of sweeps under the same computation budget computation budget $K \cdot L = 1000$.}
% \begin{tabular}{cccccc}
% \toprule
% K & 1 & 5 & 10 & 20 & 50  \\
% L & 1000 & 200 & 100 & 50 & 20 \\
% \midrule
%     $\log p_\theta(x, z)$ & -1852 & -1528 & -1423 & -1414 & -1411 \\
%     ESS / L & 0.001 & 0.011 & 0.0562  & 0.0882 & 0.295 \\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=1.0\linewidth]{figures/both-budget-dot-v2.pdf}
%   \vspace*{-3mm}
%   {\small \hspace{1em}(a) $\log p_\q(x, z)$\hspace{22em}(b) ESS / L}\\
%   \caption{APG sampler performance as a function of number of sweeps $K$ for a constant sample budget $K \cdot L = 1000$.}
%   \label{fig:budget}
% \end{figure*}
% \setlength{\tabcolsep}{4pt}
% \begin{table}[t!]
%     \centering
%     \caption{APG performance in the GMM and DGMM.
%     The left column in each table shows the change in log joint distribution, i.e. the difference between the log joint in the baseline and the log joint in other models. We compute the ESS/L metric is computed w.r.t.~different variable blocks. For the GMM we additionally report the inclusive KL (Equation~\ref{eq:variational_objective}) for each block.}
%     \begin{tabular}{cc|ccc}
     
%     \toprule
%          & 
%         \multicolumn{3}{c}{ESS/L} \\
%     \midrule
%      RWS & -- & 0.001 & --\\
%      APG (K=5)  & 0.261 & 0.980 &  0.631  \\
%      APG (K=10)  & 0.398 & 0.981  & 0.760 \\
%      APG (K=15)  & 0.416 & 0.983 & 0.780 \\
%     \bottomrule
%     \end{tabular}   
%     \label{table-gmm}
%     \begin{subtable}{.7\linewidth}
%     \centering
%     \caption{DGMM}
%     \vspace{-0.6em}
%     \begin{tabular}{c|cc}
%     \toprule
%          \multicolumn{3}{c}{ESS/L} \\
%     \{$\mu, c, \alpha $\}& \{$\mu$\} & \{$c, \alpha$\}\\
%     \midrule
%     0.001 & -- & -- \\
%      0.002 & 0.013 & 0.422 \\
%      0.002 & 0.019 & 0.454 \\
%      0.003 & 0.025 & 0.488 \\
%     \bottomrule
%     \end{tabular}
%     \label{table-dgmm}
%     \end{subtable}
%     \label{table-both-mm}
% \end{table}

% \setlength{\tabcolsep}{4pt}
% \begin{table*}[t!]
%     \centering
%     \caption{APG performance in the GMM and DGMM.
%     The left column in each table shows the change in log joint distribution, i.e. the difference between the log joint in the baseline and the log joint in other models. We compute the ESS/L metric is computed w.r.t.~different variable blocks. For the GMM we additionally report the inclusive KL (Equation~\ref{eq:variational_objective}) for each block.}
%     \begin{subtable}{.45\linewidth}
%     \centering
%     \caption{GMM}
%     \vspace{-0.6em}
%     \begin{tabular}{cc|cccc}
     
%     \toprule
%          & 
%         \multicolumn{3}{c}{ESS/L} &
%         \multicolumn{2}{c}{$\mathrm{KL}(p_\q || q_\f)$}\\
%       & \{$\tau, \mu, c$\} & \{$\tau, \mu$\} & \{$c$\} &\{$\tau, \mu$\}  & \{$c$\} \\
%     \midrule
%      RWS & -- & 0.001 & --& -- & -- \\
%      APG (K=5)  & 0.261 & 0.980 &  0.631 & 0.005 & 0.005
% \\
%      APG (K=10)  & 0.398 & 0.981  & 0.760
%      & 0.004 & 0.004
%      \\
%      APG (K=15)  & 0.416 & 0.983 & 0.780 & 0.003 & 0.004\\
%     \bottomrule
%     \end{tabular}   
%     \label{table-gmm}
%     \end{subtable}%
%     \begin{subtable}{.7\linewidth}
%     \centering
%     \caption{DGMM}
%     \vspace{-0.6em}
%     \begin{tabular}{c|cc}
%     \toprule
%          \multicolumn{3}{c}{ESS/L} \\
%     \{$\mu, c, \alpha $\}& \{$\mu$\} & \{$c, \alpha$\}\\
%     \midrule
%     0.001 & -- & -- \\
%      0.002 & 0.013 & 0.422 \\
%      0.002 & 0.019 & 0.454 \\
%      0.003 & 0.025 & 0.488 \\
%     \bottomrule
%     \end{tabular}
%     \label{table-dgmm}
%     \end{subtable}
%     \label{table-both-mm}
% \end{table*}
Results on each of these tasks constitute a significant advance relative to the state of the art. Standard VAEs perform poorly at Gaussian mixture modeling tasks, and to our knowledge there are no existing methods that scale to a problem of the complexity of the DGMM for rings. In the context of the unsupervised tracking model, APG easily scales beyond previously reported results for a specialized recurrent architecture \cite{kosiorek2018sequential}. APG is not only is able to scale to models with higher complexity in these settings, but also provides a general framework for performing inference in models with global and local variables, which can be adapted to a variety of model classes with comparative ease.
\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/convergence_gmm.pdf}
  \caption{Comparison with baselines}
  \label{fig:convergence-gmm}
\end{figure}

\subsection{Gaussian Mixture Model}
\label{sec:gmm}
\vspace{-0.5em}
To evaluate whether APG samplers can learn the exact Gibbs updates in conditionally conjugate models, we consider a Gaussian mixture model 
\begin{align*}
    \mu_i, \tau_i \sim \text{NormGamma}(\mu_0, \nu_0, \alpha_0, \beta_0)
    , i &=1,2..,I \\
    c_n \sim \mathrm{Cat}(\pi), 
    x_n | c_n\!=\!i \sim \text{Norm}(\mu_i, 1 / \tau_i)
    ,n &=1,2,..,N
\end{align*}
In this model, the global variables $z^\textsc{g} = \{\mu_{1:I}, \tau_{1:I}\}$ are the mean an precision for each mixture component, whereas the local variables are the cluster assignments $z^\textsc{l} = \{c_{1:N}\}$. Conditioned on cluster assignments, the Gaussian likelihood $p(\x_{1:N} \mid z_{1:N}, \mu_{1:I}, \tau_{1:I})$ is conjugate to a normal-gamma prior $p(\mu_{1:I}, \tau_{1:I})$ with sufficient statistics $T(x_n, c_n)$ 
\begin{align*}
    \Big\{\mathrm{I}[c_n \!=\! i], 
        ~\mathrm{I}[c_n \!=\! i] \: x_n, 
        ~\mathrm{I}[c_n \!=\! i] \: x_n^2 
        ~\Big\vert~ i \!=\! 1,2,\dots,I 
    \Big\}
    ,
\end{align*}
where $\mathrm{I}[z_n \!=\! i]$ is an indicator function that evaluates to 1 if the equality holds, and 0 otherwise.

We employ a variational distribution that updates the global variables $q_\f(\mu, \tau \mid x, c)$ and the local variables $q_\f(c \mid x, \mu, \tau)$, using point-wise neural sufficient statistics modeled after the ones in the analytical updates (see Appendix \ref{appendix:architecture} for architecture details).

We train our models on 20,000 datasets with $I = 3$ clusters and $N = 60$ data points with fixed hyperparameters ($\mu_0 = 0$, $\nu_0 = 0.1$, $\alpha_0 = 2$, $\beta_0 = 2$). We use $20$ GMM datasets per batch, $K=10$ sweeps, $L=10$ particles, and Adam ($\mathrm{lr} = 10^{-4}, \beta_1 = 0.9, \beta_2 = 0.99$) for 200,000 iterations.

We compare the APG sampler to samples from a standard encoder with MLP and LSTM architectures,  which is trained using reweighted wake-sleep (RWS). Both architectures are parameterized using the same neural sufficient statistics as the APG sampler.

Figure \ref{samples-gmm} shows sequences of single samples from the variational distribution, where the first sample is drawn using RWS. Even when using a parameterization that employs neural sufficient statistics, the RWS encoder fails to propose reasonable clusters, whereas the APG sampler typically converges within 12 iterations across a range of dataset sizes.

Furthermore, we would like to quantify how similar learned proposals $q_\f(z_b \mid x, z_{-b})$ are to the conditional posteriors $p_\q(z_b \mid x, z_{-b})$. With the case of GMM where the exact conditional posterior is tractable, we verify the convergence of the learned proposals by computing the inclusive KL divergence $\mathcal{K}_b(\f)$ defined in equation ~\ref{eq:variational_objective} (see Table \ref{table-gmm}). We can see that the APG samplers of the both $\{\tau, \mu\}$ and $\{c\}$ converge to the true conditional posterior.


% \begin{figure}[t!]
%   \centering
%   {\small \hspace{1em}\textsf{\textbf{GMM}}\hspace{10em}\textsf{\textbf{DGMM}}}\\
%   \includegraphics[width=1.0\linewidth]{figures/both-budget-dot.pdf}
% %   \vspace*{-3mm}
%   \caption{APG sampler performance as a function of number of sweeps $K$ for a constant sample budget $K \cdot L = 1000$.\\ \textbf{Top}: Log joint $\log p_\q(\x ,\z)$. \textbf{Bottom}: ESS / L.}
%   \label{fig:budget}
% \end{figure}

\subsection{Deep Generative Mixture Model}
We next consider the task of training a deep generative model $p_\q(\x, \z)$ is jointly with the APG sampler. Our dataset consists of ring-shaped clusters. The true generative model (which we assume is unknown) takes the form
\begin{align*}
    \mu_i \sim \text{Norm}(0, 
    \sigma_0^2 I), 
    \qquad i = 1, 2, ..., I \\
    c_n \sim \mathrm{Disc}(\pi), \quad \alpha_n \sim \mathrm{Unif}[0, 2\pi], \\
    x_n | c_n=i \sim \text{Norm}(g_\q(\alpha_n) + \mu_i, \Sigma_\epsilon).
\end{align*}
Here $\mu_i$ is center of the $i$th ring. Given a cluster assignment $c_n$ and an angle $\alpha_n$ we define a position on a ring, from which We sample a data point $x_n$ with 2D Gaussian noise.

% \begin{figure*}[!h]
%   \centering
% %   \begin{subfigure}[t]{1.0\textwidth}
% %   \includegraphics[width=1.0\textwidth]{figures/bmnist-3digits-samples.pdf}
% % %   \vspace*{-5mm}
% % %   \caption{3 digits.}
% %   \end{subfigure}
% %   %%%%%%%%%%%
%   \begin{subfigure}[t]{1.0\textwidth}
%   \includegraphics[width=1.0\textwidth]{figures/bmnist-4digits-samples.pdf}
% %   \vspace*{-5mm}
% %   \caption{4 digits.}
%   \end{subfigure}
%   %%%%%%%%%%%
%   \begin{subfigure}[t]{1.0\textwidth}
%   \includegraphics[width=1.0\textwidth]{figures/bmnist-5digits-samples.pdf}
% %   \vspace*{-5mm}
% %   \caption{5 digits.}
%   \end{subfigure}
%   \caption{Inferred digit trajectories and reconstructions for $D = 4$  (top) and $D = 5$ (bottom) digits for $T = 15$.}
%   \label{mnist-qualitative}
% \end{figure*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=1.0\textwidth]{figures/bmnist-5digits-samples.pdf}
%   \vspace*{-5mm}
%   \caption{5 digits.}
  \caption{Inferred digit trajectories (top) and reconstructions (bottom) for test dataset with $D = 5$ digits and $T = 15$ time steps.}
  \label{mnist-qualitative}
\end{figure*}
\begin{table}[t!]
    \centering
    \caption{$\log p_\q (x, z)$ and mean squared error on BMNIST test set with different methods.}
    \begin{tabular}{ccc}
     
    \toprule
         & $\log p_\q (x, z)$ & MSE \\
    \midrule
        VAE & -- & -- \\
        APG & -- & -- \\
        HMC & -- & -- \\
        BPS & -- & -- \\
    \bottomrule
    \end{tabular}   
\end{table}
We train our model on 20,000 datasets with $N = 200$ data points and $I = 4$ clusters with fixed hyperparameters ($\sigma_0 = 3.5$, $\Sigma_{\epsilon} = 0.2$). We use $20$ datasets per batch, $K=10$ sweeps, $L=10$ particles, and Adam ($\mathrm{lr} = 10^{-4}, \beta_1 = 0.9, \beta_2 = 0.99$) for 200,000 iterations (see Appendix \ref{appendix:architecture} for architecture details).

Once again, we compare the APG sampler with the encoders using RWS. Figure~\ref{samples-dgmm} shows individual samples analogous to the ones in Figure~\ref{samples-gmm}. The APG sampler scales to a large range of number of variables, whereas a standard encoder trained using RWS fails to produce reasonable proposals.

\subsection{Sample Quality Evaluation}
In both mixture models, we compute the log-joint distribution $\log p_\q(\x, z)$ (see Table \ref{table-both-mm}) as a function of sweep iteration to measure the convergence and the effective sample size (see Table \ref{table-both-mm}) to assess proposal quality
\begin{align}
\label{ess-eq}
    \frac{\text{ESS}}{L} 
    = 
    \frac{(\sum_{l=1}^L w^{k,l})^2}
         {L \sum_{l=1}^L (w^{k,l})^2}
    .
\end{align}


\textbf{Log joint} $\log p_\q(\x, z)$. Because the marginal $q_\f(z^k | x)$ is intractable, it is difficult to compute an lower bound or upper bound at each sweep. Here we compute the log joint in each test dataset for both the APG sampler with different number of sweeps and the RWS baselines and report the differences on average to see how much more is achieved by the APG sampler. In both models, the APG sampler gains a higher log joint compared with the encoder trained by RWS.

\textbf{ESS}. One advantage of the APG sampler that it decomposes a high dimensional sampling problem into a sequence of lower dimensional sampling problems. To show that, we compute the ESS when 1) we resample only after one sweep and 2) we resample after each block update. WE can see that the granular sampling strategy significantly improves the ESS in both cases.

\subsection{Fixed Computation Budget Analysis}
As a mean of comparing the performance of APG samplers for varying numbers of sweeps $K$, we perform an experiment in which the computation budget is fixed at $K \cdot L = 1000$ samples. Figure~\ref{fig:budget} shows $\log p_\theta(x, z)$ and ESS/L. The shaded area denotes the standard deviation over 10 runs that each comprise 5 datasets that were chosen at random. We can see that it in general, it is more effective to perform more APG sweeps $K$ with a smaller number of particles $L$, that it is to increase the particle budget. 

\subsection{Time Series Model -- Bouncing MNIST}
Finally, we apply the APG sampler to a time series model that is trained with short timescales, and evaluate its performance with longer timescales and larger numbers of latent variables.
The data $x_{1:T}$ is a sequence of images of $D$ moving MNIST digits.
Our generative model consists of global variables $\z_{1:D}^{\mathrm{what}}$ corresponding to digit latent variables and local variables $\z_{1:D, 1:T}^{\mathrm{where}}$ corresponding to the digit trajectories.
The deep generative model is a state space model that factorizes across digits of the form
\begin{align*}
    z^{\mathrm{what}}_{d} 
    &\sim 
    \text{Norm}(0, I), 
    \,\,\, 
    z^{\mathrm{where}}_{d, 1} \sim \text{Norm}(0, I), 
    \\
    z^{\mathrm{where}}_{d, t} 
    %| z^{\mathrm{where}}_{d, t - 1} 
    &\sim 
    \text{Norm}(z^{\mathrm{where}}_{d, t - 1}, \sigma^2_0 I) 
    \\
    x_t 
    %| z^\mathrm{where}_{d,t}, z^\mathrm{what}_d 
    &\sim
    \mathrm{Bern}
    \Big(
        \sigma
        \Big(
            \sum_{d} \mathrm{ST}
            \big(
                \mu_\theta(\z^{\mathrm{what}}_d), 
                z^{\mathrm{where}}_{d, t}
            \big)
        \Big)
    \Big)
\end{align*}
Here, ST is a spatial transformer \cite{jaderberg2015spatial} that maps the output of a feedforward decoder $\mu_\q$ that maps logits for a 28$\times$28 MNIST image onto a 96$\times$96 canvas based on the location variable $z^{\mathrm{where}}_{d, t}$. 

Our amortized Gibbs updates employ $T + 1$ blocks $(z_{1:D}^{\mathrm{what}}, z_{1:D, 1}^{\mathrm{where}}, z_{1:D, 2}^{\mathrm{where}}, \dotsc, z_{1:D, T}^{\mathrm{where}})$.
Empirically this works better than splitting the latent variables into global and local variables, since resampling at each time step $t$ helps disentangle the digit locations if they overlap.

We train our model on 60000 bouncing MNIST sequences, each of which contains $D=3$ digits and $T=10$ frame images. We use $10$ sequences per batch, $K=5$ sweeps, $L=10$ particles, and Adam ($\mathrm{lr} = 10^{-4}, \beta_1 = 0.9, \beta_2 = 0.99$) for 200,000 iterations (see Appendix \ref{appendix:architecture} for architecture details).

We show that APG sampler can scale to larger number of variables by testing the model on datasets with $T\in \{20, 100\}$ time steps  and $D \in \{3, 4, 5\}$ digits. Figure~\ref{mnist-qualitative} shows the inference and reconstruction using single samples from the variational distribution. (plots are truncated by the first $15$ time steps due to limited space, see Appendix \ref{appendix:full-recons} for more examples with full time steps). 
Qualitatively, we see that the digit trajectories $\z_{1:D, 1:T}^{\mathrm{where}}$ and latent variables $\z_{1:D}^{\mathrm{what}}$ are inferred well.
In Figure~\ref{plot-mse-bmnist}, we show the mean squared error between the video and its reconstruction for different $T$ and $D$.
The results confirm that performance improves with increasing number of Gibbs sweeps $K$.
In certain cases, a larger number of time points $T$ in fact improves convergence as a function of the number of sweeps $K$.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/bmnist-mse.pdf}
%     \caption{Mean squared error between video frames and reconstructions as a function of the number of APG sweeps. }
%     \label{plot-mse-bmnist}
% \end{figure}

\section{Conclusion}
One of the challenges in amortized inference for deep generative models is learning high-quality proposals for models with a structured prior over a high-dimensional set of latent variables. These priors arise naturally when, rather than encoding a single data point (e.g.~an image), we wish to encode a dataset (e.g.~a sequence of images). 
Even for apparently simple problems, such as inferring the cluster parameters and assignments in a mixture model, standard encoders often fail to produce good samples. One of the reasons for this is that it is fundamentally difficult to jointly generate proposals for a high-dimensional set of latent variables.

APG samplers are very general, and offer a path towards the development of deep generative models that incorporate structured priors to provide meaningful inductive biases in settings where we have little or no supervision. These methods have particular strengths in problems with global variables, but more generally make it possible to design amortized approaches that exploit conditional independence. Moreover, our parameterization in terms of neural sufficient statistics makes it comparatively easy to design models that scale to much larger number of latent variables and thus generalize to datasets that vary in size. 

Immediate lines of future work are to compare the approach in this paper, which learns kernels that leave the target density invariant, with approaches that perform annealing, in which the learned kernels are assymmetric in the sense that they gradually transform the initial encoder distribution to the target density. 

\section{Acknowledgements}

This work was supported by the Intel Corporation, NSF award 1835309, the DARPA LwLL program, and startup funds from Northeastern University. Tuan Anh Le was supported by AFOSR award FA9550-18-S-0003.

\bibliography{icml-2020-references}
\bibliographystyle{icml2020}

\newpage
\appendix
\onecolumn
\icmltitle{Supplementary Material: Amortized Population Gibbs Samplers with Neural Sufficient Statistics}
\icmlkeywords{Machine Learning, ICML}
% \vskip 0.3in
\section{Gradient of the generative model}% $p_\q(x \mid z)$}
\label{appendix:grad-theta}
This is actually a known (although indeed not obvious) identity. Briefly, we can express the expected gradient of the log joint as
\begin{align*}
    \mathbb{E}_{p_\q(\z | \x)} 
    \left[
    \nabla_\q \log p_\q(\x, \z)
    \right]
    \mathbb{E}_{p_\q(\z | \x)} 
    \left[
    \nabla_\q \log p_\q(\x) + \nabla_\q \log p_\q(\z | \x)
    \right]
    \mathbb{E}_{p_\q(\z | \x)} 
    \left[
    \nabla_\q \log p_\q(\x) 
    \right]
    \nabla_\q \log p_\q(\x)
\end{align*}
Here we make use of a standard identity that is also used in, e.g., likelihood-ratio estimators
\begin{align*}
\mathbb{E}_{p_\q(\z | \x)}
\left[
    \nabla_\q \log p_\q(\z | \x)
\right] 
=
\int p_\q(\z | \x) \nabla_\q \log p_\q(\z | \x) \: dz
=
\int \nabla_\q p_\q(\z | \x) \: dz
=
\nabla_\q \int p_\q(\z | \x) \: dz
=
\nabla_\q 1
= 
0
\end{align*}
Therefore, we have the the following equality
\begin{align*}
\nabla_\q \log p_\q(\x) 
= 
\mathbb{E}_{p_\q(\z | \x)} 
\left[
\nabla_\q \log p_\q(\x, \z)
\right].
\end{align*}
which is Equation ~\ref{eq:grad-theta}. As a result, we can then use self-normalized importance sampling to approximate     $\mathbb{E}_{p_\q(\z | \x)} \left[\nabla_\q \log p_\q(\x, \z) \right]$.

\section{Importance weights in sequential importance sampling}
\label{appendix:sis-weight}
At step $k=1$, we use exactly the standard importance sampler, thus it is obvious that the following is a valid importance weight
\begin{align*}
    w^1 = \frac{\gamma^1(z^1)}{q^1(z^1)}.
\end{align*}
When step $k>2$, we are going to prove that the importance weight relative to the intermediate densities has the form
\begin{align}
    \label{appendix:eq:sis-weight}
    w^k
    = 
    \frac{\gamma^k(z^{1:k})}
         {q^1(z^1) \prod_{k'=2}^k q^{k'}(z^{k'} \mid z^{1:k'-1})}.
\end{align}

At step $k=2$, the importance weight is defined as 
\begin{align*}
    w^k 
    &= 
    v^{2} \: w^1
    \: =
    \frac{\gamma^2(z^{1:2})}{\gamma^{1}(z^{1})\:q^2(z^2 \mid z^{1})} \frac{\gamma^1(z^1)}{q^1(z^1)}
    \: = \frac{\gamma^2(z^{1:2})}{q^1(z^1) \: q^2(z^2 \mid z^{1})}.
\end{align*}
which is exactly that form. Now we prove weights in future steps by induction. At step $k\geq 2$, assume the weight has the form in Equation~\ref{appendix:eq:sis-weight}, i.e.
\begin{align*}
    w^k
    = 
    \frac{\gamma^k(z^{1:k})}
         {q^1(z^1) \prod_{k'=2}^k q^{k'}(z^{k'} \mid z^{1:k'-1})}.    
\end{align*}
then at step $k+1$, the importance weight is the product of incremental weight and incoming weight 
\begin{align*}
    w^{k+1}
    =
    v^{k+1} \: w^k
    =
    \frac{\gamma^{k+1}(z^{1:k+1})}{\gamma^{k}(z^{1:k})\:q^{k+1}(z^{k+1} \mid z^{1:k})}
    \frac{\gamma^k(z^{1:k})}
         {q^1(z^1) \prod_{k'=2}^k q^{k'}(z^{k'} \mid z^{1:k'-1})}
    =
    \frac{\gamma^{k+1}(z^{1:k+1})}{q^1(z^1) \prod_{k'=2}^{k+1} q^{k'}(z^{k'} \mid z^{1:k'-1})}
    .    
\end{align*}
Thus the importance weight $w^k$ has the form of Equation~\ref{appendix:eq:sis-weight} at each step $k>2$ in sequential importance sampling.
\section{ Derivation of Posterior Invariance}
\label{appendix:posterior-invariance}
We can see that individual block updates leave the posterior invariant by proposing variables $\z^k_{\preceq b}$ from a partial kernel $\kappa(\z^k_{\preceq b} \mid x, \z^{k-1})$ and then marginalize over the corresponding variables from the previous step $\z^{k-1}_{\preceq b}$,
\begin{align*}
    \int 
    dz^{k-1}_{\preceq b} 
    \:
    p_\q(\z^{k-1} \mid \x) 
    \: 
    \kappa(\z^k_{\preceq b} \mid x, \z^{k-1}) 
    %\\
    %&= 
    &=
    \int 
    dz^{k-1}_{\preceq b} 
    \:
    p_\q(\z^{k-1} \mid \x) 
    \: 
    \int dz^k_{\succ b}
    \:
    \kappa(\z^k \mid x, \z^{k-1})
    \\
    &= 
    \int 
    dz^{k-1}_{\preceq b} 
    \: 
    p_\q(\z^{k-1} \mid x)
    \prod_{m=1}^b p_\q(\z^k_m \mid \x, \z^{k}_{\prec m}, \z^{k-1}_{\succ m})
    \\
    &=
    \int 
    dz^{k-1}_{\preceq b} 
    \: 
    p_\q(\z^{k-1} \mid \x)
    p_\q(\z^k_{\preceq b} \mid \x, \z^{k-1}_{\succ 1})
    \\
    %&=
    %\int d\z^{k-1}_{\preceq b} 
    %\: p_\q(\z^k_{\preceq b}, \z^{k-1} \mid \x)\\
    &=
    p_\q(\z^k_{\preceq b}\:, \: \z^{k-1}_{\succ b} \mid \x)
    .
\end{align*}

\section{Resampling Algorithm}
\label{appendix:resample-algo}
\begin{algorithm}[!t]
    \setstretch{1.2}
  \caption{\textsc{resample}}
  \label{alg:resample}
\begin{algorithmic}[1]
\small
  \State \textbf{Input:} $z^{\:1:L}, w^{\:1:L}$
  \State \textbf{Output:} $z'^{\:1:L}, w'^{\:1:L}$
  \For {$i = 1$ \textbf{to} $L$}
    \State $a^i \sim \mathrm{Disc}(\{w^l / \sum_{l' = 1}^L w^{l'}\}_{l=1}^L)$\Comment{Index Selection} 
    % \State $P(a^i = l) = w^l / \sum_{l' = 1}^L w^{l'}$
    \State Set $ z'^{\:i} = z^{a^i}$
    \State Set $ w'^{\:i} = \frac{1}{L} \sum_{l = 1}^L w^l$\Comment{Re-Weigh}
    \EndFor
  \State \textbf{Return} $z'^{\:1:L}, w'^{\:1:L}$
\end{algorithmic}
\end{algorithm}

\section{Proof of the amortized population Gibbs algorithm}
\label{appendix:proof-algo}

Here, we provide an alternative proof of correctness of the APG algorithm given in Algorithm~\ref{alg:amortized-gibbs}, based on the construction of proper weights~\cite{naesseth2015nested} which was introduced after SMC samplers~\cite{delmoral2006sequential}.
We first introduce proper weights, and then present several operations that preserve the proper weighting property and finally we apply these properties in proving correctness of APG.

\subsection{Proper weights}

\begin{definition}[Proper weights]
    Given an unnormalized density $\tilde p(z)$, with corresponding normalizing constant $Z_p := \int \tilde p(z) \,\mathrm dz$ and normalized density $p \equiv \tilde p / Z_p$, the random variables $z, w \sim P(z, w)$ are properly weighted with respect to $\tilde p(z)$ if and only if for any measurable function $f$
    \begin{align}
    \label{eq:pw}
    \E_{P(z, w)}\left[w f(z)\right] = Z_p \E_{p(z)}[f(z)]. 
    \end{align}
    We will also denote this as
    \begin{align*}
        z, w \pw \tilde p.
    \end{align*}
\end{definition}

\paragraph{Using proper weights.}
Given independent samples $z^l, w^l \sim P$, we can estimate $Z_p$ by setting $f \equiv 1$:
\begin{align*}
    Z_p \approx \frac{1}{L} \sum_{l = 1}^L w^l.
\end{align*}
This estimator is unbiased because it is a Monte Carlo estimator of the left hand side of \eqref{eq:pw}.
We can also estimate $\E_{p(z)}[f(z)]$ as
\begin{align*}
    \E_{p(z)}[f(z)] \approx \frac{\frac{1}{L} \sum_{l = 1}^L w^l f(z^l)}{\frac{1}{L} \sum_{l = 1}^L w^l}.
\end{align*}
While the numerator and the denominator are unbiased estimators of $Z_p \E_{p(z)}[f(z)]$ and $Z_p$ respectively, their fraction is biased.
We often write this estimator as
\begin{align}
    \E_{p(z)}[f(z)] \approx \sum_{l = 1}^L \bar w^l f(z^l), \label{eq:pw-estimation}
\end{align}
where $\bar w^l := w^l / \sum_{l' = 1}^L w^{l'}$ is the normalized weight.

\subsection{Operations that preserve proper weights}


\begin{proposition}[Nested importance sampling]
    Adapted from \cite[Algorithm 1]{naesseth2015nested}.
    Given unnormalized densities $\tilde q(z), \tilde p(z)$ with the normalizing constants $Z_q, Z_p$ and normalized densities $q(z), p(z)$, if 
    \begin{align}
        z, w \pw \tilde q, \label{eq:1}
    \end{align}
    then
    \begin{align*}
        z, \frac{w\tilde p(z)}{\tilde q(z)} \pw \tilde p.
    \end{align*}
\end{proposition}
\begin{proof}
    First define the distribution of $z, w$ as $Q$.
    For measurable $f(z)$
    \begin{align*}
        \E_{Q(z, w)}\left[\frac{w\tilde p(z)}{\tilde q(z)} f(z)\right] 
        = Z_q \E_{q(z)}\left[\frac{\tilde p(z) f(z)}{\tilde q(z)}\right]
        = Z_q \int q(z) \frac{\tilde p(z) f(z)}{\tilde q(z)} \,\mathrm dz
        = \int \tilde p\textbf{}(z) f(z) \,\mathrm dz
        = Z_p \E_{p(z)}[f(z)].
    \end{align*}
\end{proof}



\begin{proposition}[Resampling]
\label{proposition:resampling}
    Adapted from \cite[Section 3.1]{naesseth2015nested}.
    Given an unnormalized density $\tilde p(z)$ (normalizing constant $Z_p$, normalized density $p(z)$), if we have a set of properly weighted samples
    \begin{align}
        z^l, w^l \pw \tilde p,  \quad l = 1,\ldots, L \label{eq:bla}
    \end{align}
    then the resampling operation preserves the proper weighting, i.e.
    \begin{align*}
        z'^{\:l}, w'^{\:l} \pw \tilde p, \quad l = 1,\ldots, L
    \end{align*}
    where $z'^{\:l} = z^{a}$ with probability $P(a = i) = w^i / \sum_{l=1}^L w^l$ and $w'^{\:l} := \frac{1}{L} \sum_{l = 1}^L w^l$.
\end{proposition}
\begin{proof}
    Define the distribution of $z^l, w^l$ as $\hat{P}$.
    We show that for any $f$, $\E[f(z^{a}) w'^{\:l}] = Z_p \E_{p(z)}[f(z)]$.
    \begin{align*}
    &
        \E_{\left(\prod_{l=1}^L \hat{P}(z^l, w^l)\right) p(a \mid w^{1:L})} \bigg[f(z^{a}) w'^{l}\bigg] \\
        &= \E_{\prod_{l=1}^L \hat{P}(z^l, w^l)}\left[\sum_{i = 1}^L f(z^i) w' \: P(a = i)\right]  \\
        &= \E_{\prod_{l=1}^L \hat{P}(z^l, w^{l})}\left[\sum_{i = 1}^L f(z^i) w'  \frac{w^i}{ \sum_{l'=1}^L w^{l'}}\right]  \\
        &= \E_{\prod_{l=1}^L \hat{P}(z^l, w^l)}\left[\frac{1}{L}\sum_{i = 1}^L f(z^i) w^i\right] \\
        &= \frac{1}{L}\sum_{i = 1}^L \E_{\hat{P}(z^i, w^i)}\left[f(z^i) w^i\right]
        = \frac{1}{L}\sum_{i = 1}^L Z_p \E_{p(z)}[f(z)]
        = Z_p \: \E_{p(z)}[f(z)]. 
    \end{align*}
\end{proof}
Therefore, the resampling will return a new set of samples that are still properly weighted relative to the target distribution in the APG sampler (Algorithm~\ref{alg:amortized-gibbs}).
\begin{proposition}[Move]
\label{proposition:extendedspace}
    Given an unnormalized density $\tilde p(z)$ (normalizing constant $Z_p$, normalized density $p(z)$) and normalized conditional densities $q(z' \given z)$ and $r(z \given z')$, the proper weighting is preserved if we apply the transition kernel to a properly weighted sample, i.e.~if we have
    \begin{align}
        &
        z^l, w^l \pw \tilde p, \label{eq:pw-of-p}\\[1em]
        &
         z'^{\:l} \sim q(z'^{\:l} \given z^l),  \label{eq:z-prime}\\[1em]
        &
        w'^{\:l} = \frac{\tilde p(z'^{\:l})r(z^l \given z'^{\:l})}{\tilde p(z^l)q(z'^{\:l} \given z^l)} w^l, \qquad l = 1, \ldots, L \label{eq:w-prime}
    \end{align}
%     \noindent\begin{subequations} 
%     \begin{tabularx}{\textwidth}{@{}XXX@{}}
%         \begin{equation}
%             z, w \pw \tilde p, \label{eq:pw-of-p}
%         \end{equation}&
%         \begin{equation}
%             z' \sim q(z' \given z), \label{eq:z-prime}
%         \end{equation}&
%         \begin{equation}
%             w' = \frac{\tilde p(z')r(z \given z')}{\tilde p(z)q(z' \given z)} w, \label{eq:w-prime}
%         \end{equation}
%     \end{tabularx}
%   \end{subequations} 
then we have
    \begin{align}
        z'^{\:l}, w'^{\:l} \pw \tilde p, \qquad  l = 1, \ldots, L \label{eq:to-prove}
    \end{align}
\end{proposition}
\begin{proof}
    Firstly we simplify the notation by dropping the superscript $l$ without loss of generality. Define the distribution of $z, w$ as $\hat{P}$.
    Then, due to \eqref{eq:pw-of-p}, for any measurable $f(z)$, we have
    \begin{align*}
        \E_P[w f(z)] = Z_p E_{p}[f(z)].
    \end{align*}
    To prove \eqref{eq:to-prove}, we show $\E_{\hat{P}(z, w)q(z' \given z)}[w' f(z')] = Z_p \E_{p(z')}[f(z')]$ for any $f$ as follows:
    \begin{align}
        \E_{\hat{P}(z, w)q(z' \given z)}[w' f(z')]
        &= \E_{\hat{P}(z, w)q(z' \given z)}\left[\frac{\tilde p(z')r(z \given z')}{\tilde p(z)q(z' \given z)} w f(z')\right] 
        \nonumber\\
        &= \int \hat{P}(z, w)q(z' \given z) \frac{\tilde p(z')r(z \given z')}{\tilde p(z)q(z' \given z)} w f(z') \,\mathrm dz \,\mathrm dw \,\mathrm dz' 
        \nonumber\\
        &= \int \hat{P}(z, w) \frac{\tilde p(z')r(z \given z')}{\tilde p(z)} w f(z') \,\mathrm dz \,\mathrm dw \,\mathrm dz' 
        \nonumber\\
        &= \int \tilde p(z') f(z') \left(\int \hat{P}(z, w) w \frac{r(z \given z')}{\tilde p(z)}\,\mathrm dz \,\mathrm dw\right) \,\mathrm dz'
        \nonumber\\
        &= \int \tilde p(z') f(z') Z_p \E_{p(z)}\left[\frac{r(z \given z')}{\tilde p(z)}\right] \,\mathrm dz'. \label{eq:pause}
    \end{align}
    Using the fact that $\E_{p(z)}\left[\frac{r(z \given z')}{\tilde p(z)}\right] = \int p(z) \frac{r(z \given z')}{\tilde p(z)} \,\mathrm dz = \int r(z \given z') \,\mathrm dz / Z_p = 1 / Z_p$.
    Equation~\ref{eq:pause} simplifies to
    \begin{align*}
        \int \tilde p(z') f(z') \,\mathrm dz' = Z_p \E_{p(z')}[f(z')].
    \end{align*}
\end{proof}

\subsection{Correctness of APG Sampler}
We provide the proof by performing 2 steps in the APG sampler (Algorithm~\ref{alg:amortized-gibbs}), i.e,~ we prove the correctness when we initialize samples at step $k=1$ (line~\ref{line:rws-loop} - line~\ref{line:rws-grad-theta}) and then do one Gibbs sweep at step $k=2$ (line~\ref{line:apg-sweep-begin} - line~\ref{line:apg-sweep-end}). In fact, its correctness still holds if we perform more Gibbs sweeps by induction.

\textbf{Step $k=1$}. We initialize the proposal $z\sim q_\phi(z \given x)$ (line~\ref{line:rws-propose}) and train that encoder using the wake-$\phi$ phase objective in the standard reweighted wake-sleep\cite{le2019revisiting}
$\E_{p(x)}\left[\textsc{kl}\left(p_\theta(z \given x) || q_\phi(z \given x)\right)\right]$. 
Then we estimate its gradient w.r.t. parameter $\phi$ (line~\ref{line:rws-grad-phi}) as
\begin{align}
    \label{eq:g-rws-phi}
    g_\phi :&= - \nabla_\phi \: \E_{p(x)}\left[\textsc{kl}\left(p_\theta(z \given x) || q_\phi(z \given x)\right)\right] \\
    &
    =
    \E_{p(x)}\left[
    \E_{p_\theta(z \given x)}\left[\nabla_\phi \log q_\phi(z \mid x)\right]
    \right]
    . 
\end{align}

\textbf{Step $k=2$}. After one full sweep, we have the following objective
\begin{align*}
    \E_{p(x)}\Big[
    \sum_{b = 1}^B\E_{p_\theta(z_{-b} \mid x)}\left[\textsc{kl}\left(p_\theta(z_b \mid z_{-b}, x) || q_\phi(z_b \mid x, z_{-b})\right)\right]\Big] 
\end{align*}


% Then we estimate the gradient of the following objective after $K$ sweeps
% \begin{align}
%     \mathcal K(\phi) :=  \E_{p(x)}\Big[\textsc{kl}\left(p_\theta(z_{1:B} \mid x) || q_\phi(z_{1:B} \mid x)\right) 
%     + 
%     \sum_{b = 1}^B\E_{p_\theta(z_{-b} \mid x)}\left[\textsc{kl}\left(p_\theta(z_b \mid z_{-b}, x) || q_\phi(z_b \mid x, z_{-b})\right)\right]\Big] \label{eq:objective}
% \end{align}
% The first term learns an initial proposal $q_\phi(z \given x)$ and the second term (sum over $B$ terms) learns the $B$ conditionals $q_\phi(z_b \mid x, z_{-b})$.

% At the beginning of the APG algorithm, we initialize the gradient estimator $g_\f = 0$. At step $k=1$, i.e. the reweighed wake-sleep 

% line~\ref{line:rws-grad-phi}, $g_\phi$ estimates $\nabla_\phi \E_{p(x)}\left[\textsc{kl}\left(p_\theta(z_{1:B} \given x) || q_\phi(z_{1:B} \given x)\right)\right]$ as in a standard reweighted wake-sleep objective.

And we will prove that we correctly estimate the following gradient w.r.t. parameter $\phi$ at each block update (line~\ref{line:apg-grad-phi}) 
\begin{align}
    g_\phi^b 
    :&= - \nabla_\phi \E_{p(x)}\left[ \E_{p_\theta(z_{-b} \given x)}\left[\textsc{kl}\left(p_\theta(z_b \given z_{-b}, x) || q_\phi(z_b \given z_{-b}, x)\right)\right]
    \right]
    \\
%    &= \E_{p_\theta(z_{-b} \given x)}\left[\E_{p_\theta(z_b \given z_{-b}, x)}\left[  -\nabla_\phi \log q_\phi(z_b \given z_{-b}, x)\right]\right] \nonumber\\
    &= 
    \E_{p(x)}\left[
    \E_{p_\theta(z_{1:B} \given x)}\left[\nabla_\phi \log q_\phi(z_b \given z_{-b}, x)\right]
    \right], \qquad b = 1, \ldots, B
    . \label{eq:g-phi-b}
\end{align}
At each step, as long as we show that samples are properly weighted
\begin{align}
    z_{1:B}^l, w^l \pw p_\theta(z_{1:B}, x), \qquad l = 1, \ldots, L.
    \label{eq:invariant}
\end{align}
Equation~\ref{eq:pw-estimation} will guarantee the validity of both gradient estimations (line~\ref{line:rws-grad-phi} and line~\ref{line:apg-grad-phi}).

At step $k=1$, samples are properly weighted because $z^l$ and $w^l$ are proposed using importance sampling (line~\ref{line:rws-weight}) where $q_\phi(z \given x)$ is the proposal density and $p_\theta(z^l, x)$ is the unnormalized target density. The resampling step (line~\ref{line:resample}) will preserve the proper weighting because of  Proposition~\ref{proposition:resampling}.

To prove that Gibbs sweep (line~\ref{line:apg-sweep-begin} - line~\ref{line:apg-sweep-end}) in the APG sampler also preserves proper weighting, we show that each block update satisfies all the 3 conditions (Equation~\ref{eq:pw-of-p}, ~\ref{eq:w-prime} and ~\ref{eq:invariant}) in Proposition~\ref{proposition:extendedspace}, by which we can conclude the samples are still properly weighted after each block update. Without loss of generality, we drop all $l$ superscripts in the rest of the proof. Before we start any block update (before line~\ref{line:apg-propose}),  we already know that samples are properly weighted, i.e.
\begin{align}
    z, w \pw p_\theta(z, x).
\end{align}
which corresponds Equation~\ref{eq:pw-of-p}. Next we define a conditional distribution $q(z' \mid z):= q_\phi(z_b' \given x, z_{-b}) \delta_{z_{-b}}(z_{-b}')$, from which we propose a new sample
\begin{align}
    z' \sim q_\phi(z_b' \given x, z_{-b}) \delta_{z_{-b}}(z_{-b}'),
\end{align}
where the density of $z_{-b}'$ is a delta mass on $z_{-b}$ defined as $\delta_{z_{-b}}(z_{-b}') = 1$ if $z_{-b} = z_{-b}'$ and $0$ otherwise.
In fact, this sampling step is equivalent to firstly sampling $z_b' \sim q_\phi(\cdot \given x, z_{-b})$ (line~\ref{line:apg-propose}) and let $z_{-b}' = z_{-b}$, which is exactly what the APG sampler assumes procedurally. This condition corresponds to Equation~\ref{eq:z-prime}.

Finally, we define the weight $w'$
\begin{align}
    w' = \frac{{\color{blue}p_\theta(x, z_b', z_{-b}')} {\color{red}r(z_b \given x, z_{-b}) \delta_{z_{-b}}(z_{-b})}}{{\color{red}p_\theta(x, z_b, z_{-b})} {\color{blue}q_\phi(z_b' \given x, z_{-b}) \delta_{z_{-b}}(z_{-b}')}} w,
\end{align}
where the terms in blue are treated as densities (normalized or unnormalized) of $z_{1:B}'$ and the terms in red are treated as densities of $z_{1:B}$.
Since both delta mass densities evaluate to one, this weight is equal to the weight computed after each block update (line~\ref{line:apg-weight}). This condition corresponds to Equation~\ref{eq:w-prime}.

Now we can apply the conclusion \eqref{eq:to-prove} in Proposition~\ref{proposition:extendedspace} and claim 
\begin{align*}
z_{1:B}', w' \pw p_\theta(z_{1:B}', x)
.
\end{align*}
since $z_{-b} = z_{-b}'$ and $z_b = z_b'$ due to the re-assignment (line~\ref{line:apg-reassign}). Based on the fact that proper weighting is preserves at both initial step $k=1$ and the Gibbs sweep $k=2$, we have proved that both gradient estimations (line~\ref{line:rws-grad-phi} and line~\ref{line:apg-grad-phi}) are correct.

\section{Architecture of the Amortized Population Gibbs samplers}
\label{appendix:architecture}
\textbf{GMM:}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
     Layer 
     & 
    \multicolumn{2}{c}{$q_\f(\mu, \tau | x)$}
    \\
    \midrule
    Input
    & 
    \multicolumn{2}{c}{$\mathrm{Concat}[x_n\in\mathbb{R}^2]$}
    \\
    \hline
    1
    & \parbox{3cm}{\centering FC 2}
    & \parbox{3cm}{\centering FC 3 Softmax}
    \\
    \bottomrule
    \end{tabular}
    \label{arch-gmm-rws}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
     Layer 
     & 
    \multicolumn{2}{c}{$q_\f(\mu, \tau | x, c)$}
    \\
    \midrule
    Input
    & 
    \multicolumn{2}{c}{$\mathrm{Concat}[x_n\in\mathbb{R}^2, c_n\in\mathbb{R}^3]$}
    \\
    \hline
    1
    & \parbox{3cm}{\centering FC 2}
    & \parbox{3cm}{\centering FC 3 Softmax}
    \\
    \bottomrule
    \end{tabular}
    \label{arch-gmm-global}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
    \toprule
        Layer
        &
        $q_\f(c | x, \mu, \tau)$
        \\
    \midrule
    Input
    & 
    $\mathrm{Concat}[x_n\in\mathbb{R}^2, \mu_i\in\mathbb{R}^2]$\\
    \hline
    1
    & \parbox{4cm}{\centering FC 32 Tanh}\\
    \hline
    2 
    & FC 1, Intermediate Variable $o_i\in\mathbb{R}$ \\
    \hline
    3
    & $\mathrm{Concat}[o_i\in\mathbb{R}]$, Softmax ($c_n$) \\
    \bottomrule
    \end{tabular}
    \label{arch-gmm-local}
\end{table}
\newpage
\textbf{DGMM:}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
        \textbf{Layer} &\multicolumn{2}{c}{$q_\f(\mu | x)$} \\
    \midrule
    Input &\multicolumn{2}{c}{$x_n\in\mathbb{R}^2$} \\
    \hline
    1 
    & 
     \parbox{4cm}{\centering FC 32 Tanh}
    & 
    \parbox{4cm}{\centering FC 32 Tanh}\\
    \hline
    2 
    &
    \parbox{4cm}{\centering FC 16 Tanh, $v_n\in\mathbb{R}$}
    & 
    \parbox{4cm}{\centering FC 4 Softmax, $\gamma_n\in\mathbb{R}^{3}$}\\
    \hline
    3 &\multicolumn{2}{c}{$T_n := \gamma_n \tens{} v_n\in\mathbb{R}^{3\times16}$} \\
    \hline
    4 &\multicolumn{2}{c}{$\mathrm{Concat}[\sum_n^N T_n[i], \mu_0\in\mathbb{R}^2, \mathrm{Diag}(\sigma_0^2 I)\in\mathbb{R}^2], i=1,2,3,4$} \\
    \hline
    5 &\multicolumn{2}{c}{FC 2$\times$32 Tanh} \\
    \hline
    6 &\multicolumn{2}{c}{FC 2$\times$8 ($\mu_{1:I}$)} \\
    \bottomrule
    \end{tabular}
    \label{arch-dgmm-rws}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
        \textbf{Layer} &\multicolumn{2}{c}{$q_\f(\mu | z, c)$} \\
    \midrule
    Input &\multicolumn{2}{c}{$\mathrm{Concat}[x_n\in\mathbb{R}^2, c_n\in\mathbb{R}^3]$} \\
    \hline 
    1 
    & 
    \parbox{4cm}{\centering FC 32 Tanh}
    & 
    \parbox{4cm}{\centering FC 32 Tanh} \\
    \hline 
    2 
    &
    \parbox{4cm}{\centering FC 16, $v_n\in\mathbb{R}$}
    & 
    \parbox{4cm}{\centering FC 4 Softmax, $\gamma_n\in\mathbb{R}^{3}$}
    \\
    \hline
    3 & \multicolumn{2}{c}{$T_n := \gamma_n \tens{} v_n\in\mathbb{R}^{3\times16}$}  \\
    \hline
    4 & \multicolumn{2}{c}{$\mathrm{Concat}[\sum_n^N T_n[i], \mu_0\in\mathbb{R}^2, \mathrm{Diag}(\sigma_0^2 I)\in\mathbb{R}^2], i=1,2,3,4$} \\
    \hline 
    5 &\multicolumn{2}{c|}{FC 2$\times$32 Tanh} \\
    \hline
    6 &\multicolumn{2}{c|}{FC 2$\times$2 ($\mu_i$)} \\
    \bottomrule
    \end{tabular}
    \label{arch-dgmm-global}
\end{table}
\newpage
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
    \toprule
        \textbf{Layer} &
        $q_\f(c | z, \mu)$ \\
    \midrule
    Input  
    & 
    $\mathrm{Concat}[x_n\in\mathbb{R}^2, \mu_i\in\mathbb{R}^2]$\\
    \hline 
    1  
    & 
    FC 32 Tanh\\
    \hline 
    2
    & 
    \parbox{4cm}{FC 1, Intermediate Variable $o_i\in\mathbb{R}$} \\
    \hline
    3 & 
    \parbox{4cm}{$\mathrm{Concat}[o_i\in\mathbb{R}]$, Softmax ($c_n$)} \\
    \bottomrule
    \end{tabular}
    \label{arch-dgmm-assignment}
\end{table}
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
    \toprule
        \textbf{Layer} &
        $q_\f(\alpha | x, z, \mu)$ \\
    \midrule
    Input  
    & 
    $x_n - \mu_i \in\mathbb{R}^2 | z_n = i$\\
    \hline 
    1  
    & 
    FC 32 Tanh\\
    \hline 
    2
    & 
    \parbox{4cm}{FC 1 Tanh} \\
    \bottomrule
    \end{tabular}
    \label{arch-dgmm-angle}
\end{table}    
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
    \toprule
    \textbf{Layer}
    &
    $p_q(x | \mu, c, \alpha)$ \\
    \midrule
    Input
    &
    $\mathrm{Concat}[\alpha_n, c_n]\in\mathbb{R}^5$ 
    \\
    \hline
    1
    &
    FC 32 Tanh \\
    \hline
    2
    &
    FC 2 Tanh ($\mu_n$, fixed $\sigma_\epsilon$)\\
    \bottomrule
    \end{tabular}
    \label{arch-dgmm-decoder}
\end{table}

\newpage
\subsubsection*{Bouncing MNIST}
\begin{table}[H]
\centering
\label{arch-bmnist-decoder}
\begin{tabular}{c|c}
    \toprule
    \textbf{Layer} & $p_\q(x | z^{\mathrm{what}}, z^{\mathrm{where}})$ \\
    \midrule
    Input & $z^{\mathrm{what}}_i \in\mathbb{R}^{10}$
    \\
    \hline
    1 &
    FC 200 ReLU \\
    \hline
    2 &
    FC 400 ReLU \\
    \hline
    3 & digit $d_i\in\mathbb{R}^{784}$\\
    \hline
    4 & ST($d_{i}, z^{\mathrm{where}}_{i, t}) \in\mathbb{R}^{9276}, i=1...,i, t=1...,T$ \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{c|c}
    \toprule
    \textbf{Layer} & $q_\f(z^{\mathrm{what}} | z^{\mathrm{where}})$ \\
    \midrule
    Input & $x_t\in\mathbb{R}^{9276}, z^{\mathrm{where}}_{i, t}\in\mathbb{R}^{2}, i=1...,I, t=1...,T$
    \\
    \hline
    1 & 
    ST($x_t$, $z^{\mathrm{where}} _{i, t}$) $\in\mathbb{R}^{784}, i=1,..,I, t=1,...,T$  \\
    \hline
    2 &
    FC 400 ReLU \\
    \hline
    3 &
    FC 200 ReLU \\
    \hline
    4 & $z^{\mathrm{what}}_{i, t} \in\mathbb{R}^{10}, i=1,..,I, t=1,...,T$  \\
    \hline
    5 &
    Mean($z^{\mathrm{what}}_{1, t}$, $1:T$) $\in\mathbb{R}^{10}, i=1,...,I$\\
    \bottomrule
    \label{arch-bmnist-enc-what}
\end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
     \toprule
    \textbf{Layer} & $q_\f(z^{\mathrm{where}} | z^{\mathrm{what}})$ \\
    \midrule
    Input &
    $x_t\in\mathbb{R}^{9276}$, $z^{\mathrm{what}}_{i, t}, i=1,.., t=1,..,T$ \\
    \hline
    1 & 
    Conv2d($x_t$, $z^{\mathrm{what}}_{i, t}$)$\in\mathbb{R}^{4638}$, $i=1,.., t=1,..,T$ \\
    \hline
    2  &
    FC 400 Tanh \\
    \hline
    3 & $2\times$ FC 200 Tanh \\
    \hline
    4 &
    $2\times2$ Tanh 
    \\
    \bottomrule
    \end{tabular}
    \label{arch-bmnist-enc-where}
\end{table}
\newpage
\section{More Qualitative Results of Bouncing MNIST}
\label{appendix:full-recons}
The following are full reconstructions on test sets where time steps $T=100$ and number of digits $D=3, 4, 5$, respectively. In each figure, the 1st, 3rd, 5th, 7th, 9th rows show the inference results, while the other rows show the reconstruction of the series above.
\begin{figure*}[h!]
\includegraphics[width=170mm]{figures/T=100-D=3.pdf}
\caption{Full reconstruction for a video where $T=100, D=3$.}
\end{figure*}


\begin{figure*}[h!]
\includegraphics[width=170mm]{figures/T=100-D=4.pdf}
\caption{Full reconstruction for a video where $T=100, D=4$.}
\end{figure*}
\newpage
\begin{figure*}[h!]
\includegraphics[width=170mm]{figures/T=100-D=5.pdf}
\caption{Full reconstruction for a video where $T=100, D=5$.}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=170mm]{figures/T=100-D=5-2.pdf}
\caption{Full reconstruction for a video where $T=100, D=5$.}
\end{figure*}

% \appendix
% \section{Do \emph{not} have an appendix here}

% \textbf{\emph{Do not put content after the references.}}
%
% Put anything that you might normally include after the references in a separate
% supplementary file.

% We recommend that you build supplementary material in a separate document.
% If you must create one PDF and cut it up, please be careful to use a tool that
% doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
% pdftk usually works fine. 

% \textbf{Please do not use Apple's preview to cut off supplementary material.} In
% previous years it has altered margins, and created headaches at the camera-ready
% stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
